{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzN-XD89myMM",
        "outputId": "42cc4d89-4ed5-432c-a674-11f3c1dad8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, random\n",
        "import json\n",
        "import signal\n"
      ],
      "metadata": {
        "id": "UiKgbwCu1suK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'drive/MyDrive/Data'\n",
        "train_dir = data_dir+'/training'\n",
        "valid_dir = data_dir+'/validation'\n",
        "test_dir = data_dir+'/testing'"
      ],
      "metadata": {
        "id": "yh-cFC2M0sw1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [transforms.RandomRotation(30),\n",
        "     transforms.RandomResizedCrop(224),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
        ")\n",
        "\n",
        "test_valid_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "xaS8S0wO1CcC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ImageFolder(train_dir,train_transform)\n",
        "valid_data = ImageFolder(valid_dir,test_valid_transform)\n",
        "test_data = ImageFolder(test_dir,test_valid_transform)"
      ],
      "metadata": {
        "id": "RFjG3mqt3O4Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class labels:\", train_data.classes)\n",
        "print(\"Class labels:\", test_data.classes)\n",
        "print(\"Class labels:\", valid_data.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLbnoTeE6BbF",
        "outputId": "cecf4af1-771c-4192-841a-d202890fc867"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class labels: ['0.1', '0.2', '0.5', '1', '10', '2', '5']\n",
            "Class labels: ['0.1', '0.2', '0.5', '1', '10', '2', '5']\n",
            "Class labels: ['0.1', '0.2', '0.5', '1', '10', '2', '5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "valid_data_loader = data.DataLoader(valid_data, batch_size=64)\n",
        "test_data_loader = data.DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "id": "_fxS8G2k3uoE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "in_features_of_pretrained_model = model.classifier[0].in_features\n",
        "classifier_out_features = len(train_data.classes)\n",
        "\n",
        "classifier = nn.Sequential(nn.Linear(in_features=in_features_of_pretrained_model, out_features=2048, bias=True),\n",
        "                           nn.ReLU(inplace=True),\n",
        "                           nn.Dropout(p=0.2),\n",
        "                           nn.Linear(in_features=2048, out_features=classifier_out_features, bias=True),\n",
        "                           nn.LogSoftmax(dim=1)\n",
        "                          )\n",
        "\n",
        "model.classifier = classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKpLC1uD4KgF",
        "outputId": "8e0ac0a0-bee8-4d0c-d96a-5c1db8b3a05a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 106MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# specify optimizer, using only classifer params. don't want to change the rest of VGG19\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
        "\n",
        "# move device to cpu, before moving it to cuda/gpu\n",
        "device = 'cuda'\n",
        "model.to(device)\n",
        "\n",
        "# init variables for tracking loss/steps etc.\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "  step = 0\n",
        "  running_train_loss = 0\n",
        "  running_valid_loss = 0\n",
        "\n",
        "  # for each batch of images\n",
        "  for images, labels in train_data_loader:\n",
        "    step += 1\n",
        "     # turn model to train mode\n",
        "    model.train()\n",
        "    # move images and model to device\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # zeroise grad\n",
        "    optimizer.zero_grad()\n",
        "  # forward\n",
        "    outputs = model(images)\n",
        "    train_loss = criterion(outputs, labels)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    running_train_loss += train_loss.item()\n",
        "    if step % 20 == 0 or step == 1 or step == len(train_data_loader):\n",
        "      print(\"Epoch: {}/{} Batch % Complete: {:.2f}%\".format(e+1, epochs, (step)*100/len(train_data_loader)))\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    running_accuracy = 0\n",
        "    running_valid_loss = 0\n",
        "    for images, labels in valid_data_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      valid_loss = criterion(outputs, labels)\n",
        "      running_valid_loss += valid_loss.item()\n",
        "      ps = torch.exp(outputs)\n",
        "      top_p, top_class = ps.topk(1, dim=1)\n",
        "      equals = top_class == labels.view(*top_class.shape)\n",
        "      running_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "        # print stats\n",
        "  average_train_loss = running_train_loss/len(train_data_loader)\n",
        "  average_valid_loss = running_valid_loss/len(valid_data_loader)\n",
        "  accuracy = running_accuracy/len(valid_data_loader)\n",
        "  print(\"Train Loss: {:.3f}\".format(average_train_loss))\n",
        "  print(\"Valid Loss: {:.3f}\".format(average_valid_loss))\n",
        "  print(\"Accuracy: {:.3f}%\".format(accuracy*100))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaqxEW-z7XOi",
        "outputId": "194cabba-7508-493b-c3e9-d266c75326b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Batch % Complete: 7.14%\n",
            "Epoch: 1/5 Batch % Complete: 100.00%\n",
            "Train Loss: 13.794\n",
            "Valid Loss: 1.553\n",
            "Accuracy: 42.566%\n",
            "Epoch: 2/5 Batch % Complete: 7.14%\n",
            "Epoch: 2/5 Batch % Complete: 100.00%\n",
            "Train Loss: 1.397\n",
            "Valid Loss: 1.093\n",
            "Accuracy: 59.848%\n",
            "Epoch: 3/5 Batch % Complete: 7.14%\n",
            "Epoch: 3/5 Batch % Complete: 100.00%\n",
            "Train Loss: 1.066\n",
            "Valid Loss: 0.869\n",
            "Accuracy: 65.483%\n",
            "Epoch: 4/5 Batch % Complete: 7.14%\n",
            "Epoch: 4/5 Batch % Complete: 100.00%\n",
            "Train Loss: 0.798\n",
            "Valid Loss: 0.804\n",
            "Accuracy: 69.981%\n",
            "Epoch: 5/5 Batch % Complete: 7.14%\n",
            "Epoch: 5/5 Batch % Complete: 100.00%\n",
            "Train Loss: 0.678\n",
            "Valid Loss: 0.669\n",
            "Accuracy: 71.970%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.class_to_idx = train_data.class_to_idx\n",
        "checkpoint = {'classifier': model.classifier,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'epochs': epochs,\n",
        "              'optim_stat_dict': optimizer.state_dict()\n",
        "             }\n",
        "\n",
        "torch.save(checkpoint, 'drive/MyDrive/checkpoint.pth')"
      ],
      "metadata": {
        "id": "dYYOj0889oUA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # for each batch of images\n",
        "    running_accuracy = 0\n",
        "    running_loss = 0\n",
        "    for images, labels in test_data_loader:\n",
        "\n",
        "        # move images and model to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # forward\n",
        "        outputs = model(images)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # accuracy\n",
        "        ps = torch.exp(outputs)\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "average_loss = running_loss/len(valid_data_loader)\n",
        "total_accuracy = accuracy/len(valid_data_loader)\n",
        "print(\"Test Loss: {:.3f}\".format(average_loss))\n",
        "print(\"Test Accuracy: {:.3f}\".format(total_accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gINkRa5zBQlp",
        "outputId": "dcfea6b3-6b88-411e-855b-07882bfecbfc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.861\n",
            "Test Accuracy: 90.077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image_path):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    pil_image = Image.open(image_path)\n",
        "\n",
        "\n",
        "    # TODO: Process a PIL image for use in a PyTorch model\n",
        "\n",
        "    # reize\n",
        "    pil_image.resize((256,256))\n",
        "\n",
        "    # centre crop\n",
        "    width, height = pil_image.size   # Get dimensions\n",
        "    new_width, new_height = 224, 224\n",
        "\n",
        "    left = round((width - new_width)/2)\n",
        "    top = round((height - new_height)/2)\n",
        "    x_right = round(width - new_width) - left\n",
        "    x_bottom = round(height - new_height) - top\n",
        "    right = width - x_right\n",
        "    bottom = height - x_bottom\n",
        "\n",
        "    # Crop the center of the image\n",
        "    pil_image = pil_image.crop((left, top, right, bottom))\n",
        "\n",
        "    # convert colour channel from 0-255, to 0-1\n",
        "    np_image = np.array(pil_image)/255\n",
        "\n",
        "    # normalize for model\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    np_image = (np_image - mean)/std\n",
        "\n",
        "    # tranpose color channge to 1st dim\n",
        "    np_image = np_image.transpose((2 , 0, 1))\n",
        "\n",
        "    # convert to Float Tensor\n",
        "    tensor = torch.from_numpy(np_image)\n",
        "    tensor = tensor.type(torch.FloatTensor)\n",
        "\n",
        "    # return tensor\n",
        "    return tensor\n",
        ""
      ],
      "metadata": {
        "id": "lFZvzSJfBfHh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image_path, model, topk=5, device=\"cuda\"):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    image = process_image(image_path)\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    # move to device\n",
        "    image = image.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ps = torch.exp(model(image))\n",
        "\n",
        "    top_ps, top_classes = ps.topk(topk, dim=1)\n",
        "\n",
        "    return top_ps.tolist()[0], top_classes.tolist()[0]\n",
        ""
      ],
      "metadata": {
        "id": "9DqPlC7wCc-m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = predict('/content/drive/MyDrive/images_test/1dh.jpeg',model)"
      ],
      "metadata": {
        "id": "FJZGT3TACiwv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AAdPr4fHG8gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqVQljekEHk5",
        "outputId": "54960fbe-1ba2-4613-e738-5e3f8cd69d7f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([0.34837090969085693, 0.20030103623867035, 0.1602628231048584, 0.14086829125881195, 0.058729853481054306], [2, 3, 0, 1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class labels:\", train_data.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OwitWfZEJEZ",
        "outputId": "a20c5f51-199b-4fda-f1b3-28c3e8eb6e02"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class labels: ['0.1', '0.2', '0.5', '1', '10', '2', '5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2w9MOSIHCQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}